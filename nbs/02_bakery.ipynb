{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6febe163-9721-4b15-8266-2a27e361d37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9288a302-2d75-4fc4-b221-705f86345a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp loadDataBakery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf1e3ed-8793-4e8d-bead-c0a21bb3e8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from fastcore.docments import *\n",
    "from fastcore.test import *\n",
    "from fastcore.utils import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from os.path import join\n",
    "import pathlib\n",
    "import pkg_resources\n",
    "\n",
    "from datasetsDynamic.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f4ac80-295e-4787-b587-68517b1be286",
   "metadata": {},
   "source": [
    "## Load and Preprocess Bakery Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2867467-34e3-4432-a9dd-9fe7ba7a490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def loadDataBakery(testDays = 28, daysToCut = 0, normalizeDemand = True, returnXY = True):\n",
    "    \n",
    "    # LOAD RAW DATA\n",
    "    dataPath = pkg_resources.resource_stream(__name__, 'datasets/dataBakery_unprocessed.csv')\n",
    "    data = pd.read_csv(dataPath)\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # RENAME AND DROP COLUMNS\n",
    "    data.drop(columns=[\"temp_min\", \"temp_max\"], inplace=True)\n",
    "    \n",
    "    data.rename(columns={\"date_short\": \"date\", \n",
    "                         \"shop_no\": \"store\", \n",
    "                         \"product_no\": \"item\", \n",
    "                         \"temp_avg_celsius\": \"temperature\", \n",
    "                         \"rain_mm\": \"rain\"}, \n",
    "                inplace=True)\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # REMOVE INTERMITTENT DEMAND\n",
    "    data_grouped = data.groupby([\"store\", \"item\"])\n",
    "    groups = list(data_grouped.groups.keys())\n",
    "    \n",
    "    # get all store/item instances with more than 20 percent zero sales\n",
    "    more_than_20_p_zero = []\n",
    "    for group in groups:\n",
    "        data_temp = data_grouped.get_group(group)\n",
    "        zero = data_temp[data_temp[\"demand\"] == 0].shape[0]\n",
    "        non_zero = data_temp[data_temp[\"demand\"] != 0].shape[0]\n",
    "        if zero / (non_zero + zero) >= 0.2:\n",
    "            more_than_20_p_zero.append(group)\n",
    "            \n",
    "    # drop zero sales instances\n",
    "    for group in more_than_20_p_zero:\n",
    "        data = data.drop(data_grouped.get_group(group).index)\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # CALENDAR FEATURES\n",
    "    data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\n",
    "    data['year'] = data['date'].dt.year\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # ID Feature\n",
    "    data['id'] = [str(data.store.iloc[i]) + '_' + str(data.item.iloc[i]) for i in range(data.shape[0])]\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # DAY INDEX\n",
    "    data['dayIndex'] = data['date'].apply(lambda x: getDayIndex(x))\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # CUT DAYS DEPENDING ON DAYSTOCUT\n",
    "    cutOffDate = data.dayIndex.max() - daysToCut\n",
    "    data = data[data['dayIndex'] <= cutOffDate].reset_index(drop = True)\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # LABEL\n",
    "    if isinstance(testDays, int):\n",
    "        nDaysTest = testDays\n",
    "    else:\n",
    "        tsSizes = data.groupby(['id']).size()\n",
    "        nDaysTest = int(tsSizes.iloc[0] * testDays)\n",
    "\n",
    "    cutoffDateTest = data.dayIndex.max() - nDaysTest\n",
    "    data['label'] = ['train' if data.dayIndex.iloc[i] <= cutoffDateTest else 'test' for i in range(data.shape[0])]    \n",
    "\n",
    "    #---\n",
    "\n",
    "    # data = data.sort_values(by = ['id', 'dayIndex'], axis = 0).reset_index(drop = True)\n",
    "\n",
    "    #---\n",
    "\n",
    "    # NORMALIZE DEMAND\n",
    "    if normalizeDemand:\n",
    "        scalingData = data[data.label == 'train'].groupby('id')['demand'].agg('max').reset_index()\n",
    "        scalingData.rename(columns = {'demand': 'scalingValue'}, inplace = True)\n",
    "        data = pd.merge(data, scalingData, on = 'id')\n",
    "\n",
    "        data['demand'] = data.demand / data.scalingValue\n",
    "    else:\n",
    "        data['scalingValue'] = 1\n",
    "\n",
    "    #---\n",
    "\n",
    "    # DEMAND LAG FEATURES\n",
    "    \n",
    "    data = createLagFeatures(data = data, \n",
    "                             idFeature = 'id', \n",
    "                             lagDays = range(1, 8),\n",
    "                             lagDaysArithmetic = [7, 14, 21, 28])\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    data['year'] = data['year'].apply(lambda x: str(int(x)))\n",
    "\n",
    "    data = pd.concat([data, \n",
    "                      pd.get_dummies(data.weekday, prefix = 'weekday'), \n",
    "                      pd.get_dummies(data.month, prefix = 'month'), \n",
    "                      pd.get_dummies(data.year, prefix = 'year')], axis = 1).drop(['weekday', 'month', 'year'], axis = 1)\n",
    "\n",
    "    #---\n",
    "    \n",
    "    # STORE AND ITEM DUMMY VARIABLES\n",
    "    data['item'] = data['item'].apply(lambda x: str(int(x)))\n",
    "    data['store'] = data['store'].apply(lambda x: str(int(x)))\n",
    "    data = pd.concat([data, \n",
    "                      pd.get_dummies(data.item, prefix = 'item'), \n",
    "                      pd.get_dummies(data.store, prefix = 'store')], axis = 1).drop(['store', 'item'], axis = 1)\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # SPLIT INTO TRAIN AND TEST DATA\n",
    "    XArray = np.array(data.drop(['demand', 'label', 'id'], axis = 1))   \n",
    "    yArray = np.ravel(data['demand'])    \n",
    "    \n",
    "    XTrain = XArray[data['label'] == 'train']\n",
    "    yTrain = yArray[data['label'] == 'train']\n",
    "\n",
    "    XTest = XArray[data['label'] == 'test']\n",
    "    yTest = yArray[data['label'] == 'test']\n",
    "\n",
    "    #---\n",
    "    \n",
    "    if not normalizeDemand:\n",
    "        data.drop(['normalizeDemand'], axis = 1)\n",
    "\n",
    "    if returnXY:\n",
    "        return data, XTrain, yTrain, XTest, yTest\n",
    "    else:\n",
    "        return data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2852ce5e-24fd-4f8d-bc9b-3c7479d8be80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def loadDataBakery2(testDays = 28, returnXY = True, daysToCut = 0, disable_progressbar = False):\n",
    "    \n",
    "    # LOAD RAW DATA\n",
    "    dataPath = pkg_resources.resource_stream(__name__, 'datasets/dataBakery_unprocessed.csv')\n",
    "    data = pd.read_csv(dataPath)\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # RENAME AND DROP COLUMNS\n",
    "    data.drop(columns=[\"temp_min\", \"temp_max\"], inplace=True)\n",
    "    \n",
    "    data.rename(columns={\"date_short\": \"date\", \n",
    "                         \"shop_no\": \"store\", \n",
    "                         \"product_no\": \"item\", \n",
    "                         \"temp_avg_celsius\": \"temperature\", \n",
    "                         \"rain_mm\": \"rain\"}, \n",
    "                inplace=True)\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # REMOVE INTERMITTENT DEMAND\n",
    "    data_grouped = data.groupby([\"store\", \"item\"])\n",
    "    groups = list(data_grouped.groups.keys())\n",
    "    \n",
    "    # get all store/item instances with more than 20 percent zero sales\n",
    "    more_than_20_p_zero = []\n",
    "    for group in groups:\n",
    "        data_temp = data_grouped.get_group(group)\n",
    "        zero = data_temp[data_temp[\"demand\"]==0].shape[0]\n",
    "        non_zero = data_temp[data_temp[\"demand\"]!=0].shape[0]\n",
    "        if zero/(non_zero+zero) >= 0.2:\n",
    "            more_than_20_p_zero.append(group)\n",
    "            \n",
    "    # drop zero sales instances\n",
    "    for group in more_than_20_p_zero:\n",
    "        data = data.drop(data_grouped.get_group(group).index)\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # CALENDAR FEATURES\n",
    "    data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\n",
    "    data['year'] = data['date'].dt.year\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # ID Feature\n",
    "    data['id'] = [str(data.store.iloc[i]) + '_' + str(data.item.iloc[i]) for i in range(data.shape[0])]\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # DAY INDEX\n",
    "    data['dayIndex'] = data['date'].apply(lambda x: getDayIndex(x))\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # CUT DAYS DEPENDING ON DAYSTOCUT\n",
    "    cutOffDate = data.dayIndex.max() - daysToCut\n",
    "    data = data[data['dayIndex'] <= cutOffDate].reset_index(drop = True)\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # LABEL\n",
    "    if isinstance(testDays, int):\n",
    "        nDaysTest = testDays\n",
    "    else:\n",
    "        tsSizes = data.groupby(['id']).size()\n",
    "        nDaysTest = int(tsSizes.iloc[0] * testDays)\n",
    "\n",
    "    cutoffDateTest = data.dayIndex.max() - nDaysTest\n",
    "    data['label'] = ['train' if data.dayIndex.iloc[i] <= cutoffDateTest else 'test' for i in range(data.shape[0])]    \n",
    "\n",
    "    #---\n",
    "\n",
    "    # data = data.sort_values(by = ['id', 'dayIndex'], axis = 0).reset_index(drop = True)\n",
    "\n",
    "    #---\n",
    "\n",
    "    # NORMALIZE DEMAND\n",
    "    scalingData = data[data.label == 'train'].groupby('id')['demand'].agg('max').reset_index()\n",
    "    scalingData.rename(columns = {'demand': 'scalingValue'}, inplace = True)\n",
    "    data = pd.merge(data, scalingData, on = 'id')\n",
    "    \n",
    "    data['demand'] = data.demand / data.scalingValue\n",
    "\n",
    "    #---\n",
    "\n",
    "    # DEMAND LAG FEATURES\n",
    "    \n",
    "    y = pd.DataFrame(data['demand'])\n",
    "    X = data.drop(columns = ['demand'])\n",
    "\n",
    "    # set lag features\n",
    "    fc_parameters = MinimalFCParameters()\n",
    "\n",
    "    # delete length features\n",
    "    del fc_parameters['length']\n",
    "\n",
    "    # create lag features\n",
    "    X, y = add_lag_features(X = X, \n",
    "                            y = y, \n",
    "                            column_id = ['id'],\n",
    "                            column_sort = 'dayIndex', \n",
    "                            feature_dict = fc_parameters, \n",
    "                            time_windows = [(7, 7), (14, 14), (28, 28)],\n",
    "                            n_jobs = 32, \n",
    "                            disable_progressbar = False)\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    X['year'] = X['year'].apply(lambda x: str(int(x)))\n",
    "\n",
    "    X = pd.concat([X, \n",
    "                  pd.get_dummies(X.weekday, prefix = 'weekday'), \n",
    "                  pd.get_dummies(X.month, prefix = 'month'), \n",
    "                  pd.get_dummies(X.year, prefix = 'year')], axis = 1).drop(['weekday', 'month', 'year'], axis = 1)\n",
    "\n",
    "    X = pd.concat([X, pd.get_dummies(X.item, prefix = 'item')], axis = 1).drop(['item'], axis = 1)\n",
    "\n",
    "    #---\n",
    "    \n",
    "    # STORE AND ITEM DUMMY VARIABLES\n",
    "    data['item'] = data['item'].apply(lambda x: str(int(x)))\n",
    "    data['store'] = data['store'].apply(lambda x: str(int(x)))\n",
    "    dataTrain = pd.concat([data, \n",
    "                           pd.get_dummies(data.item, prefix = 'item'), \n",
    "                           pd.get_dummies(data.store, prefix = 'store')], axis = 1).drop(['store', 'item'], axis = 1)\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # SPLIT INTO TRAIN AND TEST DATA\n",
    "    data = pd.concat([y, X], axis = 1)\n",
    "    XArray = np.array(X.drop(['label', 'id'], axis = 1))   \n",
    "    yArray = np.ravel(y)    \n",
    "    \n",
    "    XTrain = XArray[data['label'] == 'train']\n",
    "    yTrain = yArray[data['label'] == 'train']\n",
    "\n",
    "    XTest = XArray[data['label'] == 'test']\n",
    "    yTest = yArray[data['label'] == 'test']\n",
    "\n",
    "    #---\n",
    "\n",
    "    if returnXY:\n",
    "        return data, XTrain, yTrain, XTest, yTest\n",
    "    else:\n",
    "        return data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba747cf3-2c08-405b-8f1a-ab7595d5e933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasets",
   "language": "python",
   "name": "datasets"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
